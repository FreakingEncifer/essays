<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        div {
            margin-bottom: 15px;
            padding: 4px 12px;
        }

        .imp {
            background-color: #ffdddd;
            border-left: 6px solid #f44336;
        }

        .ref {
            background-color: #ddffdd;
            border-left: 6px solid #4CAF50;
        }

        .info {
            background-color: #e7f3fe;
            border-left: 6px solid #2196F3;
        }

        .warning {
            background-color: #ffffcc;
            border-left: 6px solid #ffeb3b;
        }





    </style>
    <title>Machine Learning Notes</title>
</head>
<body>

<h2>Machine Learning Notes/Experiences</h2>
<h3>Collected by reading blogs</h3>

<div class="info">
    <p>
        <strong>Basic Machine Learning Problem Solving Process (BMLPSP)</strong>
        <br>
        The following content is taken from Machine Learning Mastery website
        This might be specific w.r.t. Time Series Forecasting but the concepts are general enough.
        General points to remember: Don't skip steps. Don't be sloppy. Keep the code clean and parameterized. Make the
        test-harness handy.
    <ol>
        <li>
            <strong> Define Problem: </strong> <br>
            We can find answers for the following questions with the help of Data Visualizations,
            ACF/PACF plots, domain experts, stakeholders, etc.
            <ol>
                <li>
                    What are the inputs and outputs for a forecast?
                </li>
                <li>
                    What are the endogenous and exogenous variables?
                </li>
                <li>
                    Are the time series variables unstructured or structured?
                </li>
                <li>
                    Are you working on a regression or classification predictive modeling problem?
                </li>
                <li>
                    What are some alternate ways to frame your time series forecasting problem?
                </li>
                <li>
                    Are you working on a univariate or multivariate time series problem?
                </li>
                <li>
                    Do you require a single-step or a multi-step forecast?
                </li>
                <li>
                    Do you require a static or a dynamically updated model?
                </li>
            </ol>
        </li>
        <li>
            <strong>Design Test Harness</strong><br>
            The test harness should be robust enough to estimate the model performance for comparison using some
            metrics.
            <ol>
                <li>
                    Split the dataset into a train and test set.
                </li>
                <li>
                    Fit a candidate approach on the training dataset.
                </li>
                <li>
                    Make predictions on the test set directly or using walk-forward validation.
                </li>
                <li>
                    Calculate a metric that compares the predictions to the expected values.
                </li>
            </ol>
        </li>
        <li>
            <strong>Test Models:</strong><br>
            <ol>
                <li>
                    Baseline.
                </li>
            </ol>
            Persistence (grid search the lag observation that is persisted)
            Rolling moving average.

            Autoregression.
            ARMA for stationary data.
            ARIMA for data with a trend.
            SARIMA for data with seasonality.

            Exponential Smoothing.
            Simple Smoothing
            Holt Winters Smoothing

            Linear Machine Learning.
            Linear Regression
            Ridge Regression
            Lasso Regression
            Elastic Net Regression

            Nonlinear Machine Learning.
            k-Nearest Neighbors
            Classification and Regression Trees
            Support Vector Regression

            Ensemble Machine Learning.
            Bagging
            Boosting
            Random Forest
            Gradient Boosting

            Deep Learning.
            MLP
            CNN
            LSTM
            Hybrids

        </li>
        <li>
            Finalize Model
        </li>

    </ol>

    </p>
</div>


<div class="info">
    <p>
        <strong>How to evaluate an imbalanced classifier?</strong>
        <br>
        One way to evaluate imbalanced classification models
        that predict crisp labels is to calculate the separate accuracy on the positive class and the negative class,
        referred to as sensitivity and specificity. These two measures can then be averaged using the geometric mean,
        referred to as the G-mean, that is insensitive to the skewed class distribution and correctly reports on the
        skill of the model on both classes.
    </p>
</div>


<div class="imp">
    <p>
        <strong>Formulae to evaluate an imbalanced classifier?</strong>
        <br>
        Recall that the sensitivity is a measure of the accuracy for the positive class and specificity is a measure of
        the accuracy of the negative class.
    <ul>
        <li>
            Sensitivity = TruePositives / (TruePositives + FalseNegatives)
        </li>
        <li>
            Specificity = TrueNegatives / (TrueNegatives + FalsePositives)
        </li>

    </ul>
    The G-mean seeks a balance of these scores, the geometric mean, where poor performance for one or the other results
    in a low G-mean score.
    <br>
    <strong>G-Mean = sqrt(Sensitivity * Specificity)</strong>
    </p>
</div>

<div class="info">
    <p>
        <strong>How to choose k in k-cross validation?</strong>
        <br>
        Three common tactics for choosing a value for k are as follows:
    <ol>
        <li>Representative: The value for k is chosen such that each train/test group of data samples is large enough to
            be statistically representative of the broader dataset.
        </li>
        <li>k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally
            result in a model skill estimate with low bias a modest variance.
        </li>
        <li>k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an
            opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.
        </li>
    </ol>
</div>

<div class="info">
    <p>
        <strong>Variations in k-CV</strong>
        <br>
    <ol>
        <li>Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is
            created to evaluate the model.
        </li>
        <li>LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that
            each observation is given a chance to be the held out of the dataset. This is called leave-one-out
            cross-validation, or LOOCV for short.
        </li>
        <li>Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has
            the same proportion of observations with a given categorical value, such as the class outcome value. This is
            called stratified cross-validation.
        </li>
        <li>Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the
            data sample is shuffled prior to each repetition, which results in a different split of the sample.
        </li>
    </ol>
    </p>
</div>

<div class="imp">
    <p>
        <strong>SMOTE-ENN - Synthetic Minority Over-sampling Technique</strong>
        <br>
        It is an oversampling technique for creating new synthetic examples for the minority class. This can be paired
        with the Edited Nearest Neighbor (ENN) algorithm that will locate and remove examples from the dataset that are
        ambiguous, making it easier for models to learn to discriminate between the two classes. SMOTE and ENN both work
        better when the input data is scaled beforehand. This is because both techniques involve using the nearest
        neighbor algorithm internally and this algorithm is sensitive to input variables with different scales.
        Therefore, we will require the data to be normalized as a first step, then sampled, then used as input to the
        (unbalanced) logistic regression model.
    </p>
</div>

<div class="info">
    <p>
        <strong>Explain SMOTE and ENN</strong>
        <br>
        ???
    </p>
</div>


<div class="info">
    <p>
        <strong>Explain Logistic Regression in short</strong>
        <br>
        ???
    </p>
</div>

<div class="info">
    <p>
        <strong>Types of Regression Techniques along with available libraries</strong>
        <br>
        ???
    </p>
</div>

<div class="info">
    <p>
        <strong>Types of Classification Techniques along with available libraries</strong>
        <br>
        ???
    </p>
</div>

<div class="warning">
    <p>
        <strong>Warning: Liblinear Convergence</strong>
        <br>
        ???
    </p>
</div>

<div class="info">
    <p>
        <strong>Types of Unsupervised learning Techniques along with available libraries</strong>
        <br>
        ???
    </p>
</div>

<div class="imp">
    <p>
        <strong>ReLU is better than Sigmoid</strong>
        <br>
        Because ReLU learns continuously for positive examples. max(0, x)
        Sigmoid learning curve is very small and stagnant for large number of area.
    </p>
</div>

<div class="ref">
    <p>
        <strong>References:</strong>
        <br>
    <ol>
        <li>
            <a href="https://machinelearningmastery.com/imbalanced-classification-model-to-detect-oil-spills/">
                Imbalanced Classifier - MLM
            </a>
        </li>
        <li>
            <a href="https://machinelearningmastery.com/how-to-develop-a-skilful-time-series-forecasting-model/">
                ML Time Series Forecasting - MLM
            </a>
        </li>

    </ol>
    </p>
</div>

</body>
</html>
